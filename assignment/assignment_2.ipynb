{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76S5g6Dg0l_1"
      },
      "outputs": [],
      "source": [
        "# Author: Your name\n",
        "# Email: Your email\n",
        "# Date: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lM91gE8qQqv"
      },
      "source": [
        "# **Assignment**\n",
        "1. File -> \"save a copy in Drive\" and open the copy file.\n",
        "2. Write down a question and document.\n",
        "3. Run the code and return the token<->token_id pairs.\n",
        "4. Run the model.\n",
        "5. Add one line code: Instead of returning the span answer, just return the answer starting position id and ending position id in the paragraph. \n",
        "6. Print the answer span.\n",
        "7. Save this .ipynb and submit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXHYqnKf12DA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzYf0fxzlk7A"
      },
      "outputs": [],
      "source": [
        "## load an example. The input requires question and paragraph text.\n",
        "## load an example. The input requires question and paragraph text.\n",
        "question = \"\"  ### choose a question by yourself.\n",
        "answer_text = \"\" ### choose a document by yourself (less than 512 tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3cfGXcD1vz8"
      },
      "outputs": [],
      "source": [
        "# !pip install ipykernel\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "##########################################################################################################################\n",
        "# In this task, we can choose a famous dataset, SQuAD, as the Reading Comprehension benchmark.\n",
        "# The data link is shown in here: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json .\n",
        "##########################################################################################################################  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlrXpA661o6b"
      },
      "outputs": [],
      "source": [
        "## load the pre-trained QA model and tokenizer from Huggingface\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-eLbN0t1o6c"
      },
      "outputs": [],
      "source": [
        "### transfer the token to token id\n",
        "input_ids = tokenizer.encode(question, answer_text)\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gy00lQl1o6c"
      },
      "outputs": [],
      "source": [
        "## if you want, you can change back to token for doule checking.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXSCee641o6d"
      },
      "outputs": [],
      "source": [
        "### or more dircetly, print token<->id format\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24lDV6Of1o6d"
      },
      "outputs": [],
      "source": [
        "## create segment id (important) for question (all 0) and paragraph text (all 1)\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "num_seg_a = sep_index + 1 # The number of segment A tokens includes the [SEP] token istelf.\n",
        "\n",
        "num_seg_b = len(input_ids) - num_seg_a # The remainder are segment B.\n",
        "\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b # Construct the list of 0s and 1s.\n",
        "\n",
        "assert len(segment_ids) == len(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlmwtTy_1o6e"
      },
      "outputs": [],
      "source": [
        "### most exciting code: run the BERT model!\n",
        "# Run our example through the model.\n",
        "outputs = model(torch.tensor([input_ids]),\n",
        "                             token_type_ids=torch.tensor([segment_ids]),\n",
        "                             return_dict=True) \n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QomuulL71o6e"
      },
      "outputs": [],
      "source": [
        "## let's see the output returning result.\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8hL4ruV1o6f"
      },
      "outputs": [],
      "source": [
        "### find the answer span through the logit scores and the argmax operation\n",
        "answer_start = torch.argmax(start_scores)\n",
        "answer_end = torch.argmax(end_scores)\n",
        "\n",
        "### Hint: print the answer starting position id index and answer ending position id index in the paragraph here.\n",
        "# print('answer starting position id: ', ? , ' answer ending position id: ', ?)\n",
        "\n",
        "# Combine the tokens in the answer and print it out.\n",
        "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('cse842')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8ef4ddcfefddaf7da2de0a1e78606789b9ec3f3c0cd77fe6935a111f795ee083"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
