{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f68klEdLmVit"
      },
      "outputs": [],
      "source": [
        "### author: Your name\n",
        "### email: Your email   \n",
        "### Date:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIrokjimmprA"
      },
      "outputs": [],
      "source": [
        "!pip install d2l\n",
        "!pip install matplotlib==3.0\n",
        "!pip install matplotlib_inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-KIXCPZmey7"
      },
      "outputs": [],
      "source": [
        "from d2l import torch as d2l\n",
        "import os\n",
        "import torch     \n",
        "from torch import nn  \n",
        "##########################################################################################################################\n",
        "# In this task, we choose a famous dataset, Large Movie Review Dataset (IMDB), as the Sentiment Analysis benchmark.\n",
        "# The data link is shown in here: https://ai.stanford.edu/~amaas/data/sentiment/ .\n",
        "##########################################################################################################################  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6rgNEuamq40"
      },
      "source": [
        "# **Assignment**\n",
        "DIY your own sentiment analysis model by selecting different hypermeters.\n",
        "\n",
        "1. File -> \"save a copy in Drive\" and open the copy file.\n",
        "2. Randomly choose a batch size number.\n",
        "3. Randomly choose a learning rate number.\n",
        "4. Randomly choose a epoch number to train the model.\n",
        "5. Randomly choose a dropout rate number.\n",
        "6. Two lines of codeing: Increase the number of the sentiment_analysis_mlp layers from 2 to 3. (hint: one line coding on \"__init__\", while one line coding on \"forward\")\n",
        "7. Run the model.\n",
        "8. Generate the training acc, test acc visual graph.\n",
        "9. Write a test case input sentence and return the answer (positive or negative).\n",
        "10. Save this .ipynb and submit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI708G08nJO6"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# Set up the hyper-parameters.\n",
        "print('-----------------------------start set up heper-parameter-----------------------------')\n",
        "##########################################################################################################################\n",
        "batch_size = ## randomly choose a number. e.g., 1, 2, 4, 8, 16, 32, 64, 128, ...\n",
        "\n",
        "embed_size = 100 ## fix 100\n",
        "devices = d2l.try_all_gpus()\n",
        "\n",
        "learning_rate =  ## randomly choose a number. e.g., 1e-1, 2e-1, 1e-2, 1e-3, ...\n",
        "train_epochs = ## randomly choose a number. e.g., 1, 2, 3, 4, 5, ...\n",
        "\n",
        "dropout_rate = ## randomly choose a number in the range from 0 to 1\n",
        "\n",
        "sentence = \"\"   ## write a test case by yourself.\n",
        "\n",
        "print('-----------------------------end set up heper-parameter----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs-hPt6NvtxH"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# data preprocessing\n",
        "print('-----------------------------start loading data and data preprocessing-----------------------------')\n",
        "##########################################################################################################################\n",
        "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size) ### data preprocessing will spend more time.\n",
        "print('-----------------------------end loading data and data preprocessing-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "homHraa3oaBa"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# Create the MLP model on Sentiment Analysis Task.\n",
        "##########################################################################################################################\n",
        "class Sentiment_Analysis_MLP(nn.Module):                                                     \n",
        "    def __init__(self, vocab_size, embed_size, **kwargs):\n",
        "\n",
        "        super(Sentiment_Analysis_MLP, self).__init__(**kwargs)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.sentiment_analysis_mlp_1 = nn.Linear(embed_size, embed_size)\n",
        "        ##### hint: self.sentiment_analysis_mlp_new = nn.Linear(?, ?)\n",
        "        self.sentiment_analysis_mlp_2 = nn.Linear(embed_size, 2)\n",
        "        self.relu = nn.ReLU()                \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # torch.unsqueeze \"adds\" a superficial 1 dimension to tensor (at the specified dimension), \n",
        "        # while torch.squeeze removes all superficial 1 dimensions from tensor.\n",
        "        ## embed: (bz, sentence len, emb size) ->  (bz, 1, emb size) -> (bz, emb size)\n",
        "        embed = torch.mean(self.embedding(inputs), dim=1).squeeze()  \n",
        "        outputs = self.sentiment_analysis_mlp_1(embed)\n",
        "        outputs = self.relu(outputs)\n",
        "        ##### hint: outputs = self.sentiment_analysis_mlp_new(?)\n",
        "        outputs = self.sentiment_analysis_mlp_2(self.dropout(outputs))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO5VPEweofBv"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# Initialize the weights.\n",
        "##########################################################################################################################\n",
        "def init_weights(m):\n",
        "    if type(m) in (nn.Linear, nn.Conv1d):\n",
        "        nn.init.xavier_uniform_(m.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_gNTcjgohMI"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# Instance the MLP and put the Glove embedding into the model\n",
        "print('-----------------------------start Instance the MLP model-----------------------------')\n",
        "##########################################################################################################################\n",
        "glove_embedding = d2l.TokenEmbedding(\"glove.6b.100d\") ### load embedding will spend more time.\n",
        "embeds = glove_embedding[vocab.idx_to_token]\n",
        "net = Sentiment_Analysis_MLP(len(vocab), embed_size)\n",
        "net.apply(init_weights)\n",
        "net.embedding.weight.data.copy_(embeds)\n",
        "print('-----------------------------end Instance the MLP model-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1cQ-LbNokcN"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# Train the model.\n",
        "print('-----------------------------start training process-----------------------------')\n",
        "##########################################################################################################################\n",
        "trainer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, train_epochs, devices)\n",
        "print('-----------------------------end training process-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLn7EqNZom_Y"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# Test the model.\n",
        "print('-----------------------------start testing process-----------------------------')\n",
        "##########################################################################################################################\n",
        "\n",
        "sentence_tensor = torch.tensor(vocab[sentence.split()], device=d2l.try_gpu())\n",
        "\n",
        "label = torch.argmax(\n",
        "        net(sentence_tensor.reshape(1, -1))\n",
        "    )\n",
        "if label == 1:\n",
        "    print('The result is positive!')\n",
        "else:\n",
        "    print('The result is negative!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6JkS7NWjTcd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
