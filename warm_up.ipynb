{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup project\n",
    "## author: Chen Zheng\n",
    "## 10/05/2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  [(1, 3.761059708075416), (2, 6.869411872582783), (3, 9.536138736596952), (4, 12.188688129624548), (5, 15.699926838981314)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "dataset = []\n",
    "\n",
    "ground_truth_w = 3\n",
    "dataset = []\n",
    "for i in range(1,6):\n",
    "    x = i\n",
    "    y = ground_truth_w * x + random.random() ## y = w*x + b\n",
    "    dataset.append((x,y))\n",
    "print('dataset: ', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup 1:\n",
    "\n",
    "input: a set of pairs $[(x_1,y_1), (x_2, y_2), ..., (x_n, y_n)]$\n",
    "\n",
    "output: Find the weight w that minimizes $f(w)$. $f(w)$ is the squared mean error function (object function) where $f(w) = \\frac{1}{n}\\sum_{i=1}^n(w*x_i - y)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_f_(w):\n",
    "    res = 0\n",
    "    for x, y in dataset:\n",
    "        res += (w*x - y)**2\n",
    "    return res / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f_(w):\n",
    "    res = 0\n",
    "    for x, y in dataset:\n",
    "        res += 2 * (w*x - y) * x\n",
    "    return res / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0  w:  0.06934507455057465  loss:  109.46483053542613\n",
      "iter:  50  w:  2.1384303332037344  loss:  11.991124277733523\n",
      "iter:  100  w:  2.818762688933495  loss:  1.4527660248897343\n",
      "iter:  150  w:  3.042461597577837  loss:  0.31341266561888975\n",
      "iter:  200  w:  3.1152050873277592  loss:  0.1909109587092359\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = 0\n",
    "lr = 0.001\n",
    "for iter in range(200):\n",
    "    loss = object_f_(w)\n",
    "    gradient = gradient_f_(w)\n",
    "    w = w - lr*gradient\n",
    "    if iter % 50 == 0:\n",
    "        print('iter: ', iter, ' w: ', w, ' loss: ', loss)\n",
    "    # print('iter: ', iter, ' w: ', w, ' loss: ', loss)\n",
    "print('iter: ', 200, ' w: ', w, ' loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup 2:\n",
    "\n",
    "Upgrade the the level of the difficulty:\n",
    "\n",
    "- First chage: w from a 1-D tesor to 5-D tensor.\n",
    "\n",
    "- Second change: randomly generate 100 data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ground_truth_w = np.array([1, 2, 3, 2, 1]) ### change from w = 1 to w = [1, 2, 3, 2, 1]\n",
    "dim = len(ground_truth_w)\n",
    "dataset = []\n",
    "for i in range(100):\n",
    "    x = np.random.randn(dim)\n",
    "    y = ground_truth_w.dot(x) + np.random.randn() ## y = w.dot(x) + b\n",
    "    dataset.append((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_f_(w):\n",
    "    res = 0\n",
    "    for x, y in dataset:\n",
    "        res += (w.dot(x) - y)**2 # matrix operation\n",
    "    return res / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f_(w):\n",
    "    res = 0\n",
    "    for x, y in dataset:\n",
    "        res += 2 * (w.dot(x) - y) * x # matrix operation\n",
    "    return res / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iter = 3000 ## how about 1000? 2000? In fact, You will find the surprise after more than 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0  w:  [0.00225512 0.00502654 0.00717993 0.00565515 0.00325082]  loss:  25.12105261364159\n",
      "iter:  200  w:  [0.36040528 0.78468614 1.14308039 0.87249244 0.49530255]  loss:  9.621925804514017\n",
      "iter:  400  w:  [0.58284831 1.24501579 1.84252521 1.369675   0.76746397]  loss:  4.122037800948781\n",
      "iter:  600  w:  [0.72139101 1.51783548 2.27401878 1.65430457 0.91517775]  loss:  2.15916026077389\n",
      "iter:  800  w:  [0.80798909 1.68026554 2.54071306 1.81686768 0.99325176]  loss:  1.4538762406557129\n",
      "iter:  1000  w:  [0.86236129 1.77750116 2.70586232 1.90943883 1.03292924]  loss:  1.1984354520105165\n",
      "iter:  1200  w:  [0.89668575 1.83608454 2.80832405 1.96195551 1.05185663]  loss:  1.1050481471630618\n",
      "iter:  1400  w:  [0.91849412 1.87164452 2.87201235 1.99160639 1.0598832 ]  loss:  1.0705285044961153\n",
      "iter:  1600  w:  [0.93245376 1.8934138  2.9116724  2.00824438 1.06242103]  loss:  1.0576038061344935\n",
      "iter:  1800  w:  [0.94146491 1.90686822 2.93641328 2.01750587 1.0623814 ]  loss:  1.0526923642846726\n",
      "iter:  2000  w:  [0.94733607 1.91527107 2.95187323 2.02260691 1.06128363]  loss:  1.0507942734760218\n",
      "iter:  2200  w:  [0.95119992 1.92057818 2.96154893 2.02537658 1.05988492]  loss:  1.050046791718251\n",
      "iter:  2400  w:  [0.95376971 1.92396967 2.96761317 2.02685087 1.05853459]  loss:  1.0497463089667545\n",
      "iter:  2600  w:  [0.95549743 1.92616316 2.97141868 2.02761346 1.05737057]  loss:  1.0496228378223775\n",
      "iter:  2800  w:  [0.95667163 1.92759891 2.97380924 2.02799093 1.05642667]  loss:  1.0495709338800896\n",
      "iter:  3000  w:  [0.95747477 1.92854579 2.97530623 2.02816387 1.05569283]  loss:  1.0495486793713922\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros(dim)  ### change from w = 0 to w = [0,0,0,0,0]\n",
    "lr = 0.001\n",
    "for iter in range(total_iter):\n",
    "    loss = object_f_(w)\n",
    "    gradient = gradient_f_(w)\n",
    "    w = w - lr*gradient\n",
    "    if iter % 200 == 0:\n",
    "        print('iter: ', iter, ' w: ', w, ' loss: ', loss)\n",
    "    # print('iter: ', iter, ' w: ', w, ' loss: ', loss)\n",
    "print('iter: ', total_iter, ' w: ', w, ' loss: ', loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
