{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### author: Chen Zheng\n",
    "### Date: 10/13/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengchen/anaconda/anaconda3/envs/cse842allen/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptb.train.txt has 42068 sentences.\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################\n",
    "# download the Penn Treebank dataset: https://deepai.org/dataset/penn-treebank\n",
    "##########################################################################################################################\n",
    "file = open('ptb.train.txt', 'r')\n",
    "lines = file.readlines()\n",
    "dataset = [sentence.split() for sentence in lines]\n",
    "file.close()\n",
    "print('ptb.train.txt has {} sentences.'.format(len(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering, the vocabulary size of The dataset is:  9999\n",
      "After filtering, the vocabulary size of The dataset is:  9858\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "word_and_frequency_dict = collections.Counter([word for sentence in dataset for word in sentence]) ## key: word.  value: word appearimg times in the dataset\n",
    "print('Before filtering, the vocabulary size of The dataset is: ', len(word_and_frequency_dict.keys()))\n",
    "word_and_frequency_dict = dict(filter(lambda word: word[1] >= 5, word_and_frequency_dict.items())) ## select the words that appears at least 5 times.\n",
    "print('After filtering, the vocabulary size of The dataset is: ', len(word_and_frequency_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# create word index table\n",
    "##########################################################################################################################\n",
    "index_to_word = list(word_and_frequency_dict.keys())\n",
    "word_to_index = dict()\n",
    "for index, word in enumerate(index_to_word):\n",
    "    word_to_index[word] = index\n",
    "# print(word_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# dataset words: string to index\n",
    "##########################################################################################################################\n",
    "dataset = [[word_to_index[word] for word in sentence if word in word_to_index] for sentence in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 886963 center words.\n",
      "We have 886963 surrounding word list.\n",
      "example  200 : center word:  7  surrounding words:  [121, 1, 122, 57, 123, 124]\n",
      "example  201 : center word:  57  surrounding words:  [1, 122, 7, 123, 124, 7]\n",
      "example  202 : center word:  123  surrounding words:  [122, 7, 57, 124, 7, 51]\n",
      "example  203 : center word:  124  surrounding words:  [7, 57, 123, 7, 51, 88]\n",
      "example  204 : center word:  7  surrounding words:  [57, 123, 124, 51, 88, 125]\n",
      "example  205 : center word:  51  surrounding words:  [123, 124, 7, 88, 125, 17]\n",
      "example  206 : center word:  88  surrounding words:  [124, 7, 51, 125, 17, 113]\n",
      "example  207 : center word:  125  surrounding words:  [7, 51, 88, 17, 113, 126]\n",
      "example  208 : center word:  17  surrounding words:  [51, 88, 125, 113, 126, 127]\n",
      "example  209 : center word:  113  surrounding words:  [88, 125, 17, 126, 127, 128]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################\n",
    "# extract center_word, and surrounding_words.\n",
    "# e.g.: Hello world I love pytorch.  window size = 2\n",
    "# center_word = 'hello', surrounding_words = ['world', 'I']\n",
    "# center_word = 'world', surrounding_words = ['hello', 'I']\n",
    "# center_word = 'I', surrounding_words = ['hello', 'world', 'love', 'pytorch']\n",
    "# center_word = 'love', surrounding_words = ['world', 'I', 'pytorch']\n",
    "# center_word = 'pytorch', surrounding_words = ['I', 'love']\n",
    "##########################################################################################################################\n",
    "window_size = 3 ### tips: you can try window_size = 4, 5, ...\n",
    "center_word_list, surrounding_words_list = [], []\n",
    "for sentence in dataset:\n",
    "    if len(sentence) < 2:  \n",
    "        continue\n",
    "    center_word_list += sentence\n",
    "    for center_word_i in range(len(sentence)):\n",
    "        ### before: at most window_size, after: at most window_size. max total: window_size + window_size for each center word\n",
    "        indices = list(range(max(0, center_word_i - window_size), min(len(sentence), center_word_i + 1 + window_size)))\n",
    "        indices.remove(center_word_i) ### 'I'\n",
    "        surrounding_words_list.append([sentence[index] for index in indices]) ### ['hello', 'world', 'love', 'pytorch']\n",
    "print('We have {} center words.'.format(len(center_word_list)))\n",
    "print('We have {} surrounding word list.'.format(len(surrounding_words_list)))\n",
    "example_id = 200\n",
    "for i in range(10):\n",
    "    print('example ', example_id+i, ': center word: ', center_word_list[example_id+i], ' surrounding words: ', surrounding_words_list[example_id+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "# negative sampling. \n",
    "# for each surrounding_words, we randomly add some 'noises'.\n",
    "#########################################################################################################################\n",
    "K = 5 ## for each center word, we construct 1 positive and k negative surrounding word list pairs\n",
    "sampling_weights = [word_and_frequency_dict[word]**0.75 for word in index_to_word]\n",
    "negative_samplings_list = []\n",
    "negative_candidates = []\n",
    "count = 0\n",
    "population = list(range(len(sampling_weights)))\n",
    "for cur_sur_words in surrounding_words_list:\n",
    "    negatives = []\n",
    "    while len(negatives) < len(cur_sur_words) * K:\n",
    "        if count == len(negative_candidates):\n",
    "            negative_candidates = random.choices(population, sampling_weights, k=int(1e5))\n",
    "            count = 0\n",
    "        neg= negative_candidates[count]\n",
    "        count = count + 1\n",
    "        if neg not in set(cur_sur_words):\n",
    "            negatives.append(neg)\n",
    "    negative_samplings_list.append(negatives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# dataset preprocessing\n",
    "##########################################################################################################################\n",
    "class W_2_V_Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, center_word_list, surrounding_words_list, negative_samplings_list):\n",
    "        self.center_word_list = center_word_list\n",
    "        self.surrounding_words_list = surrounding_words_list\n",
    "        self.negative_samplings_list = negative_samplings_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.center_word_list[index], self.surrounding_words_list[index], self.negative_samplings_list[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.center_word_list)\n",
    "\n",
    "def set_up_batch_data(data):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "\n",
    "batch_size = 128\n",
    "w2v_data = W_2_V_Data(center_word_list, surrounding_words_list, negative_samplings_list)\n",
    "data_loader = Data.DataLoader(w2v_data, batch_size, shuffle=True, collate_fn=set_up_batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# model\n",
    "##########################################################################################################################\n",
    "class W2V_skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(W2V_skipgram, self).__init__()\n",
    "        self.emb_1 = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        self.emb_2 = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "        \n",
    "    def forward(self, emb1, emb2):\n",
    "        center = self.emb_1(emb1) ## torch.Size([batch size, 1, 100])\n",
    "        surround = self.emb_2(emb2) ## torch.Size([batch size, (K+1)* (window_size * 2), 100])\n",
    "        output = torch.bmm(center, surround.permute(0, 2, 1)) \n",
    "        ### [batch size, 1, 100] bmm [batch size, 100, (K+1)* (window_size * 2)] -> [batch size, 1, (K+1)* (window_size * 2)]\n",
    "        return output\n",
    "\n",
    "\n",
    "##########################################################################################################################\n",
    "# loss function: extension of the binary_cross_entropy_with_logits\n",
    "##########################################################################################################################\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.6673071845957383.\n",
      "epoch: 2, loss: 0.41197761281314177.\n",
      "epoch: 3, loss: 0.39991736626212215.\n",
      "epoch: 4, loss: 0.39462959344510906.\n",
      "epoch: 5, loss: 0.3913673517550913.\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################\n",
    "# train the word2vec skipgram model\n",
    "##########################################################################################################################\n",
    "num_epochs = 5\n",
    "lr = 0.01\n",
    "loss_fun = SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocab_size = len(index_to_word)\n",
    "embedding_size = 100\n",
    "net = W2V_skipgram(vocab_size, embedding_size)\n",
    "net = net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "for epoch in range(num_epochs):\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for batch in data_loader:\n",
    "        center, context_negative, mask, label = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
    "\n",
    "        pred = net(center, context_negative)\n",
    "\n",
    "        loss = (loss_fun(pred.view(label.shape), label, mask) *\n",
    "                mask.shape[1] / mask.float().sum(dim=1)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.cpu().item()\n",
    "        n += 1\n",
    "    print('epoch: {}, loss: {}.'.format(epoch + 1, loss_sum / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.501: performer\n",
      "cosine sim=0.473: spacecraft\n",
      "cosine sim=0.460: cells\n",
      "cosine sim=0.435: taxi\n",
      "cosine sim=0.417: sandinistas\n",
      "cosine sim=0.409: eye\n",
      "cosine sim=0.406: drug\n",
      "cosine sim=0.401: movement\n",
      "cosine sim=0.398: remedy\n",
      "cosine sim=0.397: brain\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[word_to_index[query_token]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (index_to_word[i])))\n",
    "\n",
    "get_similar_tokens('dog', 10, net.emb_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cse842allen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b16092397c7a0545235e188fb20924a691f78d32e51d93771cd971bfc1cc6a62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
